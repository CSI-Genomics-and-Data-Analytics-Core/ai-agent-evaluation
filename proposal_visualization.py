#!/usr/bin/env python3
"""
Proposal Visualization - Single High-Impact Figure
Creates a professional, publication-ready visualization showing the dramatic
improvement from integrating MCP tools with LLMs for GDC database queries.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

def load_all_data():
    """Load all CSV evaluation files"""
    csv_dir = Path('csv')
    all_data = []
    
    for csv_file in csv_dir.glob('*.csv'):
        df = pd.read_csv(csv_file)
        
        # Extract framework name from filename
        filename = csv_file.stem
        if 'LLM + RAG + MCP Tools' in filename:
            framework = 'LLM + RAG + MCP'
        elif 'LLM + MCP Tools' in filename:
            framework = 'LLM + MCP'
        elif 'Sonnet 4.5' in filename:
            framework = 'Sonnet 4.5'
        elif 'GPT 4.1' in filename:
            framework = 'GPT 4.1'
        elif 'Gemini 2.5 Pro' in filename:
            framework = 'Gemini 2.5 Pro'
        else:
            framework = filename
        
        df['Framework'] = framework
        all_data.append(df)
    
    combined_df = pd.concat(all_data, ignore_index=True)
    return combined_df

def create_proposal_figure():
    """
    Create a single comprehensive figure for proposal showing:
    - Overall correctness comparison (bar chart)
    - Correctness by query complexity (grouped bars)
    - Helpfulness scores (side panel)
    
    This provides the complete story in one professional figure.
    """
    df = load_all_data()
    
    # Define framework order and colors
    framework_order = [
        'Sonnet 4.5',
        'GPT 4.1', 
        'Gemini 2.5 Pro',
        'LLM + MCP',
        'LLM + RAG + MCP'
    ]
    
    # Display names with model info
    display_names = {
        'Sonnet 4.5': 'Sonnet 4.5\n(LLM Only)',
        'GPT 4.1': 'GPT 4.1\n(LLM Only)',
        'Gemini 2.5 Pro': 'Gemini 2.5 Pro\n(LLM Only)',
        'LLM + MCP': 'Sonnet 4.5\n+ MCP Tools',
        'LLM + RAG + MCP': 'Sonnet 4.5\n+ RAG + MCP'
    }
    
    # Color scheme: LLM-only (red shades), Tool-enhanced (blue/teal shades)
    colors = {
        'Sonnet 4.5': '#e74c3c',       # Red
        'GPT 4.1': '#c0392b',          # Dark red
        'Gemini 2.5 Pro': '#f39c12',   # Orange
        'LLM + MCP': '#3498db',        # Blue
        'LLM + RAG + MCP': '#1abc9c'   # Teal
    }
    
    # Create figure with 3 subplots
    fig = plt.figure(figsize=(20, 12))
    gs = fig.add_gridspec(2, 2, hspace=0.45, wspace=0.35, 
                          left=0.08, right=0.97, top=0.86, bottom=0.14)
    
    # Main title
    fig.suptitle('GDC Database Agent Evaluation: Impact of Tool Integration',
                 fontsize=18, fontweight='bold', y=0.94)
    
    # Subtitle
    fig.text(0.5, 0.905, 
             'Comparison of LLM-only vs. Tool-Enhanced Approaches (30 queries across 3 complexity levels)',
             ha='center', fontsize=11, style='italic', color='#555555')
    
    # ==================== SUBPLOT 1: Overall Correctness ====================
    ax1 = fig.add_subplot(gs[0, :])
    
    overall_stats = df.groupby('Framework').agg({
        'correctness': ['mean', 'sum', 'count']
    }).reset_index()
    overall_stats.columns = ['Framework', 'correctness_rate', 'correct_count', 'total']
    overall_stats['correctness_pct'] = overall_stats['correctness_rate'] * 100
    
    # Sort by framework order
    overall_stats['Framework'] = pd.Categorical(
        overall_stats['Framework'], 
        categories=framework_order, 
        ordered=True
    )
    overall_stats = overall_stats.sort_values('Framework')
    
    bars = ax1.bar(range(len(overall_stats)), 
                   overall_stats['correctness_pct'],
                   color=[colors[f] for f in overall_stats['Framework']],
                   edgecolor='black',
                   linewidth=1.5,
                   alpha=0.85)
    
    # Add value labels on bars
    for i, (bar, row) in enumerate(zip(bars, overall_stats.itertuples())):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 2,
                f'{height:.1f}%\n({int(row.correct_count)}/{int(row.total)})',
                ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    ax1.set_ylabel('Correctness Rate (%)', fontsize=13, fontweight='bold')
    ax1.set_xlabel('Framework Configuration', fontsize=13, fontweight='bold')
    ax1.set_title('A. Overall Performance: Queries Answered Correctly', 
                  fontsize=14, fontweight='bold', pad=15)
    ax1.set_xticks(range(len(overall_stats)))
    ax1.set_xticklabels([display_names[f] for f in overall_stats['Framework']], 
                        fontsize=11, ha='center')
    ax1.set_ylim(0, 105)
    ax1.grid(axis='y', alpha=0.3, linestyle='--')
    ax1.axhline(y=50, color='red', linestyle='--', linewidth=1, alpha=0.5, label='50% threshold')
    
    # Add legend for approach types
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor='#e74c3c', edgecolor='black', label='LLM Only (No Tools)', alpha=0.85),
        Patch(facecolor='#3498db', edgecolor='black', label='Tool-Enhanced', alpha=0.85)
    ]
    ax1.legend(handles=legend_elements, loc='upper left', fontsize=11, framealpha=0.95)
    
    # ==================== SUBPLOT 2: By Complexity ====================
    ax2 = fig.add_subplot(gs[1, 0])
    
    complexity_stats = df.groupby(['Framework', 'complexity'])['correctness'].mean().reset_index()
    complexity_stats['correctness_pct'] = complexity_stats['correctness'] * 100
    
    # Sort by framework order
    complexity_stats['Framework'] = pd.Categorical(
        complexity_stats['Framework'],
        categories=framework_order,
        ordered=True
    )
    complexity_stats = complexity_stats.sort_values('Framework')
    
    # Pivot for grouped bar chart
    pivot_data = complexity_stats.pivot(index='Framework', 
                                        columns='complexity', 
                                        values='correctness_pct')
    
    x = np.arange(len(pivot_data))
    width = 0.25
    complexity_colors = {'Low': '#2ecc71', 'Medium': '#f39c12', 'High': '#e74c3c'}
    
    for i, complexity in enumerate(['Low', 'Medium', 'High']):
        if complexity in pivot_data.columns:
            values = pivot_data[complexity].fillna(0)
            bars = ax2.bar(x + i*width, values, width, 
                          label=complexity,
                          color=complexity_colors[complexity],
                          edgecolor='black',
                          linewidth=1,
                          alpha=0.85)
            
            # Add value labels
            for j, (bar, val) in enumerate(zip(bars, values)):
                if val > 0:
                    ax2.text(bar.get_x() + bar.get_width()/2., val + 2,
                            f'{val:.0f}%',
                            ha='center', va='bottom', fontsize=9, fontweight='bold')
    
    ax2.set_ylabel('Correctness Rate (%)', fontsize=12, fontweight='bold')
    ax2.set_xlabel('Framework Configuration', fontsize=12, fontweight='bold')
    ax2.set_title('B. Performance by Query Complexity', 
                  fontsize=13, fontweight='bold', pad=12)
    ax2.set_xticks(x + width)
    ax2.set_xticklabels([display_names[f] for f in pivot_data.index], 
                        fontsize=10, ha='center')
    ax2.set_ylim(0, 110)
    ax2.legend(title='Complexity', fontsize=10, title_fontsize=11, loc='upper left')
    ax2.grid(axis='y', alpha=0.3, linestyle='--')
    
    # ==================== SUBPLOT 3: Helpfulness Scores ====================
    ax3 = fig.add_subplot(gs[1, 1])
    
    helpfulness_stats = df.groupby('Framework').agg({
        'helpfulness': ['mean', 'std']
    }).reset_index()
    helpfulness_stats.columns = ['Framework', 'helpfulness_mean', 'helpfulness_std']
    
    # Sort by framework order
    helpfulness_stats['Framework'] = pd.Categorical(
        helpfulness_stats['Framework'],
        categories=framework_order,
        ordered=True
    )
    helpfulness_stats = helpfulness_stats.sort_values('Framework')
    
    bars = ax3.bar(range(len(helpfulness_stats)),
                   helpfulness_stats['helpfulness_mean'],
                   yerr=helpfulness_stats['helpfulness_std'],
                   color=[colors[f] for f in helpfulness_stats['Framework']],
                   edgecolor='black',
                   linewidth=1.5,
                   alpha=0.85,
                   capsize=5,
                   error_kw={'linewidth': 2, 'ecolor': '#555555'})
    
    # Add value labels
    for bar, val in zip(bars, helpfulness_stats['helpfulness_mean']):
        ax3.text(bar.get_x() + bar.get_width()/2., val + 0.15,
                f'{val:.2f}',
                ha='center', va='bottom', fontsize=11, fontweight='bold')
    
    ax3.set_ylabel('Helpfulness Score (1-5)', fontsize=12, fontweight='bold')
    ax3.set_xlabel('Framework Configuration', fontsize=12, fontweight='bold')
    ax3.set_title('C. Response Quality Assessment', 
                  fontsize=13, fontweight='bold', pad=12)
    ax3.set_xticks(range(len(helpfulness_stats)))
    ax3.set_xticklabels([display_names[f] for f in helpfulness_stats['Framework']], 
                        fontsize=10, ha='center')
    ax3.set_ylim(0, 5.5)
    ax3.axhline(y=3, color='orange', linestyle='--', linewidth=1, alpha=0.5, 
                label='Moderate Quality (3.0)')
    ax3.axhline(y=4, color='green', linestyle='--', linewidth=1, alpha=0.5,
                label='High Quality (4.0)')
    ax3.legend(fontsize=9, loc='upper left')
    ax3.grid(axis='y', alpha=0.3, linestyle='--')
    
    # Add footer with key findings
    footer_text = (
        "Key Findings: Tool integration (using Sonnet 4.5) improves correctness from 0-10% (LLM-only) to 87% (tool-enhanced), "
        "with helpfulness scores increasing from ~1.9 to ~4.4 out of 5. "
        "Tool-enhanced approaches maintain >90% accuracy on low/medium complexity queries and 70% on high complexity."
    )
    fig.text(0.5, 0.055, footer_text,
             ha='center', fontsize=10, style='italic', 
             bbox=dict(boxstyle='round,pad=0.8', facecolor='#f0f0f0', alpha=0.8),
             wrap=True)
    
    # Save figure
    output_dir = Path('visualizations')
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / 'proposal_figure.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"\nâœ… Proposal figure saved: {output_path}")
    print(f"   Resolution: 300 DPI (publication quality)")
    print(f"   Format: PNG with white background")
    
    plt.show()

if __name__ == '__main__':
    print("Generating proposal visualization...")
    print("=" * 60)
    create_proposal_figure()
    print("=" * 60)
    print("\nðŸŽ¯ Recommendation: This single figure tells your complete story:")
    print("   â€¢ Shows dramatic performance gap (0-10% â†’ 87%)")
    print("   â€¢ Demonstrates consistency across complexity levels")
    print("   â€¢ Validates quality with helpfulness metrics")
    print("   â€¢ Professional layout ready for proposals/publications")
